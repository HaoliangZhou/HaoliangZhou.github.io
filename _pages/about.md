---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}


<h1>Welcome to <span class="accent-text">Haoliang Zhou (å‘¨æµ©æ¨‘)</span>â€™s homepage</h1>

# ğŸ§â€â™‚ï¸ Biography
<span class='anchor' id='about-me'></span>
I am currently a second-year Ph.D. student at <a href="https://i-mac-lab.net" class="link-accent">IMAC Lab</a>, <a href="https://www.tjut.edu.cn" class="link-accent">Tianjin University of Technology</a>, where I am supervised by Prof. <a href="https://ia.cas.cn/rcdw/jcqn/202404/t20240422_7129942.html" class="link-accent">Changsheng Xu (å¾å¸¸èƒœ, å›½å®¶æ°é’, IEEE/IAPR/CCF Fellow)</a>, and Prof. <a href="https://cs.tjut.edu.cn/info/1226/1795.htm" class="link-accent">Feifei Zhang (å¼ é£é£)</a>.
Before that, I received the B.S. and M.S. degree in the <a href="https://www.just.edu.cn/" class="link-accent">Jiangsu University of Science and Technology</a>, advised by Prof. <a href="https://mypage.just.edu.cn/jsjkxgc/hsc/list.htm" class="link-accent">Shucheng Huang (é»„æ ‘æˆ)</a> in 2021 and 2024, respectively.

**Research Interests**: My primary research interests are centered around multimodal learning and computer vision, specifically focusing on: Composed Image Retrieval, Multimodal Large Language Model, Test-Time Adaptation, Continual Learning, and Facial Expression Recognition.



<div class="quote-accent">
My primary research interests are centered around <span class="accent-text">Multimodal Learning & Computer Vision</span>, specifically focusing on: 
  <ul>
    <li><span class="primary-gradient-text">Composed Image Retrieval</span></li>
    <li><span class="primary-gradient-text">Multimodal Large Language Model</span></li>
    <li><span class="primary-gradient-text">Test-Time Adaptation & Continual Learning</span></li>
    <li><span class="primary-gradient-text">Facial/Micro Expression Recognition</span></li>
  </ul>
</div>



<span class='anchor' id='-news'></span>
# ğŸ”¥ News
- *2026.02*: &nbsp;ğŸ‰ğŸ‰ One paper is accepted by the <span class="accent-text">IEEE TMM</span> journal. _(SCI, Q1, IF=9.7, CCF-A)_
- *2026.01*: &nbsp;ğŸ‰ğŸ‰ I am supported by China Association for Science and Technology <span class="accent-text">Talent Cultivation Project</span>.
- *2025.12*: &nbsp;ğŸ‰ğŸ‰ One paper is accepted by the <span class="accent-text">Artificial Intelligence Review</span> journal. _(SCI, Q1, IF=13.9)_
- *2025.11*: &nbsp;ğŸ‰ğŸ‰ One paper is accepted by the <span class="accent-text">AAAI 2026</span> conference. _(CCF-A)_
- *2025.10*: &nbsp;ğŸ‰ğŸ‰ One paper is accepted by the <span class="accent-text">IEEE TIP</span> journal. _(SCI, Q1, IF=13.7, CCF-A)_
- *2024.12*: &nbsp;ğŸ‰ğŸ‰ Two papers are accepted by the <span class="accent-text">Neurocomputing</span> journal. _(SCI, Q1, IF=6.5)_
- *2024.07*: &nbsp;ğŸ‰ğŸ‰ One paper is accepted by the <span class="accent-text">IEEE TCSVT</span> journal. _(SCI, Q1, IF=11.1, CCF-B)_
- *2023.12*: &nbsp;ğŸ‰ğŸ‰ I am awarded <span class="accent-text">National Scholarship</span>.




<span class='anchor' id='-publications'></span>
# ğŸ“ƒ Publications

<div id="publications-wrapper">
  <div id="filter-container"></div>
  
  <div class='paper-box floating-card' data-tags="First Author, Test-Time Reinforcement Learning, Composed Image Retrieval, CCF-A">
    <div class='paper-box-image'>
      <div class="badge pulse-accent">AAAI 2026</div>
      <img src='/images/ttrldr.png' alt="TTRLDR Overview" width="100%">
    </div>
    <div class='paper-box-text'>
      <h3>Duplex Rewards Optimization for Test-Time Composed Image Retrieval</h3>
      <div class="authors"><span class="primary-gradient-text">Haoliang Zhou</span>â­ï¸, Feifei Zhang, Changsheng Xu</div>
      <div class="venue">The Fortieth AAAI Conference on Artificial Intelligence (AAAI 2026)</div>
      <div class="links">
        <a href="https://github.com/HaoliangZhou/TT-RLDR" class="btn-accent"><i class="fas fa-file-alt"></i> Paper</a>
        <a href="https://github.com/HaoliangZhou/TT-RLDR" class="btn-accent"><i class="fab fa-github"></i> Code</a>
      </div>
    </div>
  </div>

  <div class='paper-box floating-card' data-tags="Zero-shot Learning, Composed Image Retrieval, JCR Q1, CCF-A">
    <div class='paper-box-image'>
      <div class="badge pulse-accent">IEEE TMM 2026</div>
      <img src='/images/mmcir.png' alt="MMCIR Overview" width="100%">
    </div>
    <div class='paper-box-text'>
      <h3>Enhancing Representation Inversion and Alignment for Zero-Shot Composed Image Retrieval</h3>
      <div class="authors">Feifei Zhang, Jia Chen, <span class="primary-gradient-text">Haoliang Zhou</span>, Changsheng Xu</div>
      <div class="venue">IEEE Transactions on Multimedia (TMM 2026), SCI, JCR Q1, IF=9.7, CCF-A</div>
      <div class="links">
        <a href="https://" class="btn-accent"><i class="fas fa-file-alt"></i> Paper</a>
        <!-- <a href="https://" class="btn-accent"><i class="fab fa-github"></i> Code</a> -->
      </div>
    </div>
  </div>

  <div class='paper-box floating-card' data-tags="Facial Expression Recognition, Transfer Learning, JCR Q1">
    <div class='paper-box-image'>
      <div class="badge pulse-accent">AI Review 2026</div>
      <img src='/images/eaclip.png' alt="EACLIP Overview" width="100%">
    </div>
    <div class='paper-box-text'>
      <h3>Emotion-aware Adaptation of CLIP model for Facial Expression Recognition</h3>
      <div class="authors">Jing Huan, Mingxing Li, <span class="primary-gradient-text">Haoliang Zhou</span></div>
      <div class="venue">Artificial Intelligence Review (2025), SCI, JCR Q1, IF=13.9</div>
      <div class="links">
        <a href="https://link.springer.com/article/10.1007/s10462-025-11468-4" class="btn-accent"><i class="fas fa-file-alt"></i> Paper</a>
        <!-- <a href="https://" class="btn-accent"><i class="fab fa-github"></i> Code</a> -->
      </div>
    </div>
  </div>
  
  <div class='paper-box floating-card' data-tags="First Author, Continual Learning, Composed Image Retrieval, JCR Q1, CCF-A">
    <div class='paper-box-image'>
      <div class="badge pulse-accent">IEEE TIP 2025</div>
      <img src='/images/u2car.png' alt="U2CAR Overview" width="100%">
    </div>
    <div class='paper-box-text'>
      <h3>Dual Uncertainty-aware Correspondence Adapting and Retaining for Continual Composed Image Retrieval</h3>
      <div class="authors"><span class="primary-gradient-text">Haoliang Zhou</span>â­ï¸, Feifei Zhang, Changsheng Xu</div>
      <div class="venue">IEEE Transactions on Image Processing (TIP 2025), SCI, JCR Q1, IF=13.7, CCF-A</div>
      <div class="links">
        <a href="https://doi.org/10.1109/TIP.2025.3628454" class="btn-accent"><i class="fas fa-file-alt"></i> Paper</a>
        <a href="https://github.com/HaoliangZhou/U2CAR" class="btn-accent"><i class="fab fa-github"></i> Code</a>
      </div>
    </div>
  </div>

  <div class='paper-box floating-card' data-tags="Micro Expression Recognition, JCR Q1">
    <div class='paper-box-image'>
      <div class="badge pulse-accent">NEUCOM 2025</div>
      <img src='/images/psn.png' alt="PSN Overview" width="100%">
    </div>
    <div class='paper-box-text'>
      <h3>PSN: Parallel Spatiotemporal Network to Recognize Micro-Expression</h3>
      <div class="authors">Jingting Li, Sujing Wang, Yong Wang, <span class="primary-gradient-text">Haoliang Zhou</span>, Xiaolan Fu</div>
      <div class="venue">Neurocomputing (2025), SCI, JCR Q1, IF=6.5</div>
      <div class="links">
        <a href="https://doi.org/10.1016/j.neucom.2025.129891" class="btn-accent"><i class="fas fa-file-alt"></i> Paper</a>
        <!-- <a href="https://" class="btn-accent"><i class="fab fa-github"></i> Code</a> -->
      </div>
    </div>
  </div>
  
  <div class='paper-box floating-card' data-tags="First Author, Facial Expression Recognition, Transfer Learning, JCR Q1">
    <div class='paper-box-image'>
      <div class="badge pulse-accent">IEEE TCSVT 2024</div>
      <img src='/images/ceprompt.png' alt="CEPrompt Overview" width="100%">
    </div>
    <div class='paper-box-text'>
      <h3>CEPrompt: Cross-Modal Emotion-Aware Prompting for Facial Expression Recognition</h3>
      <div class="authors"><span class="primary-gradient-text">Haoliang Zhou</span>â­ï¸, Shucheng Huang, Feifei Zhang, Changsheng Xu</div>
      <div class="venue">IEEE Transactions on Circuits and Systems for Video Technology (TCSVT 2024), SCI, JCR Q1, IF=11.1, CCF-B</div>
      <div class="links">
        <a href="https://doi.org/10.1109/TCSVT.2024.3424777" class="btn-accent"><i class="fas fa-file-alt"></i> Paper</a>
        <a href="https://github.com/HaoliangZhou/CEPrompt" class="btn-accent"><i class="fab fa-github"></i> Code</a>
      </div>
    </div>
  </div>
  
  <div class='paper-box floating-card' data-tags="First Author, Facial Expression Recognition, Evidential Deep Learning, JCR Q1">
    <div class='paper-box-image'>
      <div class="badge pulse-accent">NEUCOM 2024</div>
      <img src='/images/uafer.png' alt="UAFER Overview" width="100%">
    </div>
    <div class='paper-box-text'>
      <h3>UA-FER: Uncertainty-aware Representation Learning for Facial Expression Recognition</h3>
      <div class="authors"><span class="primary-gradient-text">Haoliang Zhou</span>â­ï¸, Shucheng Huang, Yuqiao Xu</div>
      <div class="venue">Neurocomputing (2024), SCI, JCR Q1, IF=6.5</div>
      <div class="links">
        <a href="https://doi.org/10.1016/j.neucom.2024.129261" class="btn-accent"><i class="fas fa-file-alt"></i> Paper</a>
        <a href="https://github.com/HaoliangZhou/UAFER" class="btn-accent"><i class="fab fa-github"></i> Code</a>
      </div>
    </div>
  </div>

  <div class='paper-box floating-card' data-tags="Micro Expression Recognition, Self-supervised Learning, JCR Q1">
    <div class='paper-box-image'>
      <div class="badge pulse-accent">NEUCOM 2024</div>
      <img src='/images/dvscl.png' alt="DVSCL Overview" width="100%">
    </div>
    <div class='paper-box-text'>
      <h3>Micro-Expression Recognition using Dual-View Self-Supervised Contrastive Learning with Intensity Perception</h3>
      <div class="authors">Jingting Li, <span class="primary-gradient-text">Haoliang Zhou</span>, Yu Qian, Zizhao Dong, Sujing Wang</div>
      <div class="venue">Neurocomputing (2024), SCI, JCR Q1, IF=6.5</div>
      <div class="links">
        <a href="https://doi.org/10.1016/j.neucom.2024.129142" class="btn-accent"><i class="fas fa-file-alt"></i> Paper</a>
        <a href="https://github.com/MELABIPCAS/DVSCL" class="btn-accent"><i class="fab fa-github"></i> Code</a>
      </div>
    </div>
  </div>


  <div class='paper-box floating-card' data-tags="Continual Learning, Few-shot Learning, JCR Q1">
    <div class='paper-box-image'>
      <div class="badge pulse-accent">MMSJ 2024</div>
      <img src='images/caclip.png' alt="INCEPTR Overview" width="100%">
    </div>
    <div class='paper-box-text'>
      <h3>CA-CLIP: Category-aware Adaptation of CLIP Model for Few-Shot Class-Incremental Learning</h3>
      <div class="authors">Yuqiao Xu, <span class="primary-gradient-text">Haoliang Zhou</span>â­ï¸, Shucheng Huang</div>
      <div class="venue">Multimedia Systems (2024), SCI, JCR Q1, IF=3.5</div>
      <div class="links">
        <a href="https://link.springer.com/article/10.1007/s00530-024-01322-y" class="btn-accent"><i class="fas fa-file-alt"></i> Paper</a>
        <!-- <a href="https://" class="btn-accent"><i class="fab fa-github"></i> Code</a> -->
      </div>
    </div>
  </div>
  
  <div class='paper-box floating-card' data-tags="First Author, Micro Expression Recognition, JCR Q1">
    <div class='paper-box-image'>
      <div class="badge pulse-accent">MMSJ 2023</div>
      <img src='images/inceptr.png' alt="INCEPTR Overview" width="100%">
    </div>
    <div class='paper-box-text'>
      <h3>IncepTR: Micro-Expression Recognition Integrating Inception-CBAM and Vision Transformer</h3>
      <div class="authors"><span class="primary-gradient-text">Haoliang Zhou</span>â­ï¸, Shucheng Huang, Yuqiao Xu</div>
      <div class="venue">Multimedia Systems (2023), SCI, JCR Q1, IF=3.5</div>
      <div class="links">
        <a href="https://doi.org/10.1007/s00530-023-01164-0" class="btn-accent"><i class="fas fa-file-alt"></i> Paper</a>
        <a href="https://github.com/HaoliangZhou/IncepTR" class="btn-accent"><i class="fab fa-github"></i> Code</a>
      </div>
    </div>
  </div>
  
  <div class='paper-box floating-card' data-tags="First Author, Micro Expression Recognition">
    <div class='paper-box-image'>
      <div class="badge pulse-accent">Entropy 2023</div>
      <img src='images/dualatme.png' alt="DUALATME Overview" width="100%">
    </div>
    <div class='paper-box-text'>
      <h3>Dual-ATME: Dual-branch Attention Network for Micro-Expression Recognition</h3>
      <div class="authors"><span class="primary-gradient-text">Haoliang Zhou</span>â­ï¸, Shucheng Huang, Jingting Li, Sujing Wang</div>
      <div class="venue">Entropy (2023), SCI, JCR Q2, IF=2.1</div>
      <div class="links">
        <a href="https://doi.org/10.3390/e25030460" class="btn-accent"><i class="fas fa-file-alt"></i> Paper</a>
        <a href="https://github.com/HaoliangZhou/Dual-ATME" class="btn-accent"><i class="fab fa-github"></i> Code</a>
      </div>
    </div>
  </div>
</div>
  


<span class='anchor' id='-awards'></span>
# ğŸ† Honors and Awards
- *2025.12 - 2027.12*: China Association for Science and Technology, <span class="primary-gradient-text">Talent Cultivation Project</span> for Excellent PhD Students.
- *2024.12*  Tianjin University of Technology, First-Class Academic Scholarship.
- *2024.06*  Jiangsu University of Science and Technology, Outstanding Graduate (Top 3%), First-Class Academic Scholarship.
- *2023.12*  <span class="primary-gradient-text">China National Scholarship</span> (Top 3%).
- *2023.10*  Jiangsu University of Science and Technology, Outstanding Postgraduate Pacesetter (Top 1.5%), First-Class Academic Scholarship.
- *2022.10*  Jiangsu Province Postgraduate Mathematical Modeling Competition, Provincial Third Prize.
- *2022.10*  China University Computer Competition Network Technology Challenge, Provincial Third Prize.
- *2021.06*  Jiangsu University of Science and Technology, Outstanding Graduate (Top 3%), First-Class Academic Scholarship.



<span class='anchor' id='-educations'></span>
# ğŸ« Educations
- *2024.09 - Now*, Ph.D. Tianjin University of Technology, Computer Science and Technology. 
- *2021.09 - 2024.06*, M.S. Jiangsu University of Science and Technology, Software Engineering. 
- *2018.09 - 2019.01*, V.S. California State University San Bernardino, Management Information System. 
- *2017.09 - 2021.06*, B.S. Jiangsu University of Science and Technology, Information Management and Information System.



<span class='anchor' id='-interships'></span>
# ğŸ’¼ Internships
- *2024.09-Now*, <a href="https://nlpr-web.ia.ac.cn/mmc/index.html" class="link-accent">Multimedia Computing Group (MMC)</a>, MAIS, Institute of Automation, Chinese Academy of Sciences. 
  - Working on: Composed Image Retrieval, Multimodal Large Language Model, Test-Time Adaptation, Continual Learning
  - Advisers: Prof. <a href="https://ia.cas.cn/rcdw/jcqn/202404/t20240422_7129942.html" class="link-accent">Changsheng Xu (å¾å¸¸èƒœ, IEEE/IAPR/CCF Fellow)</a> and Prof. <a href="https://cs.tjut.edu.cn/info/1226/1795.htm" class="link-accent">Feifei Zhang (å¼ é£é£)</a>
- *2023.03-2024.06*, <a href="https://i-mac-lab.net/" class="link-accent">Intelligent Media Analysis and Computation Lab (IMAC Lab)</a>, Tianjin University of Technology.
<!-- - *2023.03-2024.09*, [Multimedia Computing Group (MMC)](https://nlpr-web.ia.ac.cn/mmc/index.html), MAIS, Institute of Automation, Chinese Academy of Sciences. -->
  - Working on: Facial Expression Recognition, Vision-Language Model, Multimedia Computing
  - Advisers: Prof. <a href="https://ia.cas.cn/rcdw/jcqn/202404/t20240422_7129942.html" class="link-accent">Changsheng Xu (å¾å¸¸èƒœ, IEEE/IAPR/CCF Fellow)</a> and Prof. <a href="https://cs.tjut.edu.cn/info/1226/1795.htm" class="link-accent">Feifei Zhang (å¼ é£é£)</a>
- *2021.07-2023.03*, <a href="http://melab.psych.ac.cn/index.html" class="link-accent">Micro-Expression Laboratory (MELAB)</a>, Institute of Psychology, Chinese Academy of Sciences.
  - Worked on: Micro-Expression Recognition, Contrastive Learning, Self-Supervised Learning
  - Advisers: Associate Prof. <a href="https://psych.cas.cn/sourcedb/cn/expert/201704/t20170411_6369874.html" class="link-accent">Su-jing Wang (ç‹ç”¦è)</a>, Associate Prof. <a href="https://psych.cas.cn/sourcedb/cn/expert/202209/t20220905_6508737.html" class="link-accent">Jingting Li (æå©§å©·)</a>



<span class='anchor' id='-services'></span>
# ğŸ‘“ Services and Activities
- **Journal Reviewer**: IEEE TPAMI/TCSVT/TMM, MMSJ, SPIC, AJSE, SIVP
- **Conference Reviewer**: IEEE CVPR/ICCV/ICME, AAAI, ACM MM, ChinaMM
- **Invited Talk**: <a href="https://www.koushare.com/live/details/49962" class="link-accent">CSIG 20th Student Member Sharing Forum</a>



<span class='anchor' id='-interests'></span>
# ğŸ“° Other Project
- *2022.03-2023.03*, Micro Expression Recognition based on Transformer and Cross Modal Feature Fusion
  - Jiangsu Postgraduate Scientific Research Innovation Plan Project, <span class="primary-gradient-text">Project Leader</span>
- *2024.01-2027.12*, Incremental Retrieval for Cross-Media Query
  - National Natural Science Foundation of China, Member 
- *2023.01-2026.12*, Pedestrian Detection via Robust Object Appearance Modeling
  - National Natural Science Foundation of China, Member
- *2018.01-2021.02*, Visual Tracking via Robust Object Appearance Modeling
  - National Natural Science Foundation of China, Member
- *2019.06-2020.06*, MVC-based Software Design and Development of Employment App
  - Jiangsu Student Innovation and Entrepreneurship Program, Member




<span class='anchor' id='-visitor'></span>
# ğŸ—ºï¸ Visitor Map
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=55a2e5&w=100&t=tt&d=q0lMQbv42GO9Uz2DKyDiIher0PBDh8IwLw2ozMawjng&co=ffffff&cmo=0c8759&cmn=e5153d&ct=979bad'></script>


<script>
document.addEventListener('DOMContentLoaded', function() {
  const wrapper = document.getElementById('publications-wrapper');
  if (!wrapper) return;

  const filterContainer = document.getElementById('filter-container');
  const paperBoxes = wrapper.querySelectorAll('.paper-box');
  
  let tagCounts = {}; 
  let activeTags = new Set();


  // 1. å®šä¹‰ä½ æƒ³è¦åœ¨é¡µé¢ä¸Šæ˜¾ç¤ºçš„ç™½åå•æ ‡ç­¾
  const visibleTagsWhitelist = ["CCF-A", "JCR Q1", "First Author", "Composed Image Retrieval", "Continual Learning", "Facial Expression Recognition", "Micro Expression Recognition"];

  // åˆå§‹åŒ–ï¼šç”Ÿæˆæ ‡ç­¾å¹¶ç»Ÿè®¡æ•°é‡
  paperBoxes.forEach(box => {
    const tagsAttribute = box.getAttribute('data-tags');
    if (tagsAttribute) {
      const tagsList = tagsAttribute.split(',').map(t => t.trim()).filter(t => t);
      
      // --- ä¿®æ”¹ï¼šåªå±•ç¤ºç™½åå•å†…çš„æ ‡ç­¾åˆ° Links ä¸Šæ–¹ ---
      const textContainer = box.querySelector('.paper-box-text');
      const linksContainer = box.querySelector('.links');
      
      if (textContainer && !textContainer.querySelector('.badge-container')) {
        const badgeContainer = document.createElement('div');
        badgeContainer.className = 'badge-container';
        
        // å…³é”®æ”¹åŠ¨ï¼šå¢åŠ äº† filter è¿‡æ»¤
        tagsList
          .filter(tag => visibleTagsWhitelist.includes(tag)) 
          .forEach(tag => {
            const badge = document.createElement('span');
            badge.className = 'inner-tag-badge';
            badge.textContent = tag;
            badgeContainer.appendChild(badge);
          });
        
        if (linksContainer) {
          textContainer.insertBefore(badgeContainer, linksContainer);
        } else {
          textContainer.appendChild(badgeContainer);
        }
      }
      // --- ä¿®æ”¹ç»Ÿè®¡é€»è¾‘ï¼šåªç»Ÿè®¡ç™½åå•å†…çš„æ ‡ç­¾ç”¨äºç”Ÿæˆé¡¶éƒ¨æŒ‰é’® ---
      allTagsList.forEach(tag => {
        if (visibleTagsWhitelist.includes(tag)) {
          tagCounts[tag] = (tagCounts[tag] || 0) + 1;
        }
      });
    }
  });

  // ç”Ÿæˆé¡¶éƒ¨è¿‡æ»¤æŒ‰é’®
  const sortedTags = Object.keys(tagCounts).sort();
  if (filterContainer) {
    filterContainer.innerHTML = ''; 
    sortedTags.forEach(tag => {
      const btn = document.createElement('button');
      btn.className = 'filter-btn';
      btn.textContent = `${tag} (${tagCounts[tag]})`;
      
      btn.onclick = () => {
        if (activeTags.has(tag)) {
          activeTags.delete(tag);
          btn.classList.remove('active');
        } else {
          activeTags.add(tag);
          btn.classList.add('active');
        }
        filterPapers(); // ç‚¹å‡»åè§¦å‘è¿‡æ»¤å’Œé«˜äº®æ›´æ–°
      };
      
      filterContainer.appendChild(btn);
    });
  }

  // ğŸ”¥ æ ¸å¿ƒé€»è¾‘æ›´æ–°ï¼šè¿‡æ»¤è®ºæ–‡ + é«˜äº®æ ‡ç­¾
  function filterPapers() {
    paperBoxes.forEach(box => {
      // 1. å¤„ç†å¡ç‰‡æ˜¾ç¤º/éšè—
      const boxTagsString = box.getAttribute('data-tags');
      const boxTags = boxTagsString ? boxTagsString.split(',').map(t => t.trim()) : [];
      
      let isVisible = true;
      if (activeTags.size > 0) {
        if (boxTags.length === 0) {
          isVisible = false;
        } else {
          // å¿…é¡»åŒ…å«æ‰€æœ‰é€‰ä¸­çš„æ ‡ç­¾ (AND é€»è¾‘)
          isVisible = Array.from(activeTags).every(activeTag => boxTags.includes(activeTag));
        }
      }

      if (isVisible) {
        box.classList.remove('hidden');
      } else {
        box.classList.add('hidden');
      }

      // 2. ğŸ”¥ å¤„ç†å†…éƒ¨æ ‡ç­¾çš„é«˜äº® (å³ä¾¿å¡ç‰‡éšè—äº†ï¼Œé€»è¾‘ä¸Šä¹Ÿæ›´æ–°ä¸€ä¸‹ï¼Œæ²¡åå¤„)
      const innerBadges = box.querySelectorAll('.inner-tag-badge');
      innerBadges.forEach(badge => {
        // å¦‚æœè¿™ä¸ªå°æ ‡ç­¾çš„æ–‡å­—ï¼Œå­˜åœ¨äº activeTags (é¡¶éƒ¨é€‰ä¸­çš„é›†åˆ) ä¸­ï¼Œå°±å˜è‰²
        if (activeTags.has(badge.textContent)) {
          badge.classList.add('active');
        } else {
          badge.classList.remove('active');
        }
      });
    });
  }
});
</script>
