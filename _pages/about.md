---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>
My name is <span class="accent-text">Gao Le Dai</span> (you can call me David or, perhaps, DVD). I'm currently a PhD student at the <a href="https://idm.pku.edu.cn/" class="link-accent">National Engineering Research Center of Visual Technology (NERC)</a> in <img src='images/pkulogo.png' style="height:1em; vertical-align:middle;"> **Peking University**. I got my Bachelor's degree in Biology from <img src='images/utlogo.png' style="height:1em; vertical-align:middle;"> **University of Toronto**. My supervisors are Assistant Professor <a href="https://www.shanghangzhang.com/" class="link-accent">Shanghang Zhang</a> and Professor <a href="https://idm.pku.edu.cn/info/1017/1040.htm" class="link-accent">Tiejun Huang</a>.

<div class="quote-accent">
My research focuses on <span class="accent-text">AI for Life Science & NeuroAI</span>: 
  <ul>
    <li>Using powerful <span class="primary-gradient-text">AI tools</span> to solve complex scientific problems.</li>
    <li>Using unique <span class="primary-gradient-text">scientific data</span> to explore irregular model dynamic</li>
  </ul>
</div>

Feel free to reach out if you'd like to discuss research or explore potential collaboration!

<div class="highlight-blocks">
  <div class="highlight-block floating-card">
    <h3><i class="fas fa-microscope"></i> AI Researcher</h3>
    <ul>
      <li>Technical preference: <span class="primary-gradient-text">Representation Learning & Phenotypical Research</span></li>
      <li>Published as first author in <span class="primary-gradient-text">Nature Methods</span>, <span class="primary-gradient-text">Nature Compuational Science</span>, <span class="primary-gradient-text">ICLR</span>, <span class="primary-gradient-text">NeurIPS</span>, <span class="primary-gradient-text">ICML</span></li>
    </ul>
  </div>
  
  <div class="highlight-block floating-card">
    <h3><i class="fas fa-pen-fancy"></i> Life Experiencer</h3>
    <ul>
      <li>Enjoy <span class="primary-gradient-text">Animates & Games</span>ğŸ¬ğŸ®; <span class="primary-gradient-text">Musics & Illustrations</span>ğŸµğŸ¨; <span class="primary-gradient-text">Sports & Car Cultures</span>ğŸ‹ï¸ğŸï¸</li>
      <li>Sharing my <span class="primary-gradient-text">research progress and POV</span> on <a href="https://www.xiaohongshu.com/user/profile/661ea8a20000000007007964" class="link-accent">Xiaohongshu (Redbook)</a></li>
    </ul>
  </div>
  
  <div class="highlight-block floating-card">
    <h3><i class="fas fa-globe-asia"></i> World Explorer</h3>
    <ul>
      <li>Visited <span class="primary-gradient-text">17 countries</span>ğŸ‡¨ğŸ‡³ğŸ‡¯ğŸ‡µğŸ‡»ğŸ‡³ğŸ‡®ğŸ‡©ğŸ‡²ğŸ‡¾ğŸ‡©ğŸ‡ªğŸ‡«ğŸ‡·ğŸ‡®ğŸ‡¹ğŸ‡ªğŸ‡¸ğŸ‡µğŸ‡¹ğŸ‡¦ğŸ‡¹ğŸ‡¨ğŸ‡­ğŸ‡¸ğŸ‡ªğŸ‡©ğŸ‡°ğŸ‡³ğŸ‡±ğŸ‡ºğŸ‡¸ğŸ‡¨ğŸ‡¦</li>
      <li>Life path: <span class="primary-gradient-text">Chengdu > Calgary > Toronto > Shenzhen > Beijing > What's Next?</span></li>
    </ul>
  </div>
</div>

<span class='anchor' id='-news'></span>
# ğŸ”¥ News
- *2026.01*: &nbsp;ğŸ‰ 2 papers (1xfirst, 1xcontribute) are accepted by <span class="accent-text">The Fourteenth International Conference on Learning Representations (ICLR 2026)</span>. See you in Rio de JaneiroğŸ‡§ğŸ‡·!
- *2025.11*: &nbsp;ğŸ‰ 2 papers (1xco-first/oral, 1xcontribute) are accepted by <span class="accent-text">The Fortieth AAAI Conference on Artificial Intelligence (AAAI 2026)</span>. See you in SingaporeğŸ‡¸ğŸ‡¬!
- *2025.10*: &nbsp;ğŸ¤ Invited talk in The 2nd AI for Science PhD Seminar hold by <span class="accent-text">The School of AI for Science, Peking University</span>. See you in ShenzhenğŸ‡¨ğŸ‡³! 
- *2025.09*: &nbsp;ğŸ‰ 2 papers (1xfirst/spotlight, 1xcontribute) are accepted by <span class="accent-text">The Thirty-ninth Annual Conference on Neural Information Processing Systems (NeurIPS 2025)</span>. See you in San DiegoğŸ‡ºğŸ‡¸! 
- *2025.07*: &nbsp;ğŸ‰ 1 papers (1xfirst) is accepted in principle by <span class="accent-text">Nature Computational Science</span>. 
- *2025.05*: &nbsp;ğŸ‰ 1 papers (1xfirst) is accepted by <span class="accent-text">The Forty-second International Conference on Machine Learning (ICML 2025)</span>. See you in VancouverğŸ‡¨ğŸ‡¦!
- *2024.12*: &nbsp;âœ… Received approval of Pilot Program of Research Fund for Excellent PhD Students hold by <span class="accent-text">Natural Science Foundation of China (NSFC)</span>. 
- *2024.07*: &nbsp;ğŸ‰ 1 papers (1xstudent first) is accepted in principle by <span class="accent-text">Nature Methods</span>. 
- *2024.01*: &nbsp;ğŸ¤ Invited talk in The Dagstuhl Seminar hold by <span class="accent-text">Leibniz Institute for Analytical Sciences (ISAS)</span>. See you in DagstuhlğŸ‡©ğŸ‡ª! 

<span class='anchor' id='-educations'></span>
# ğŸ« Educations
- *2022.09 - Present*: &nbsp;PhD Candidate in Computer Science, Peking University <img src='images/pkulogo.png' style="height:1em; vertical-align:middle;">.
- *2018.09 - 2022.06*: &nbsp;Bachelor of Science in Biology, <span class="primary-gradient-text">with Distinction</span>, University of Toronto <img src='images/utlogo.png' style="height:1em; vertical-align:middle;">.

<span class='anchor' id='-publications'></span>
# ğŸ“ƒ Publications

<div id="filter-container"></div>

<div class='paper-box floating-card' data-tags="First/Co-First Author, Representation Learning, NeuroAI, Conference">
  <div class='paper-box-image'>
    <div class="badge pulse-accent">ICLR 2026 Poster</div>
    <img src='images/spikegen.png' alt="SpikeGen Overview" width="100%">
  </div>
  <div class='paper-box-text'>
    <h3>SpikeGen: Decoupled â€œRods and Conesâ€ Visual Representation Processing with Latent Generative Framework</h3>
    <div class="authors"><span class="primary-gradient-text">Gaole Dai</span>â­ï¸, Menghang Dongâ­ï¸, Rongyu Zhangâ­ï¸, Ruichuan An, Shanghang ZhangğŸ“§, Tiejun HuangğŸ“§</div>
    <div class="venue">The Fourteenth International Conference on Learning Representations (ICLR 2026)</div>
    <div class="links">
      <a href="https://openreview.net/pdf/16538e2435f34c57dec3047289b0111c358eae62.pdf" class="btn-accent"><i class="fas fa-file-alt"></i> Paper</a>
      <a href="https://github.com/daviddaiiiii/SpikeGen" class="btn-accent"><i class="fab fa-github"></i> Code</a>
    </div>
  </div>
</div>

<div class='paper-box floating-card' data-tags="Robotics, NeuroAI, Conference">
  <div class='paper-box-image'>
    <div class="badge pulse-accent">ICLR 2026 Poster</div>
    <img src='images/spikepingpong.png' alt="SpikePingpong Overview" width="100%">
  </div>
  <div class='paper-box-text'>
    <h3>Spike Vision-based Fast-Slow Pingpong Robot System</h3>
    <div class="authors">Hao Wangâ­ï¸, Chengkai Houâ­ï¸, Xianglong Liâ­ï¸, Yankai Fu, Chenxuan Li, Ning Chen, <span class="primary-gradient-text">Gaole Dai</span>, Jiaming Liu, Tiejun Huang, Shanghang ZhangğŸ“§</div>
    <div class="venue">The Fourteenth International Conference on Learning Representations (ICLR 2026)</div>
    <div class="links">
      <a href="https://openreview.net/forum?id=d08yOXs1Dl" class="btn-accent"><i class="fas fa-file-alt"></i> Paper</a>
      <a href="https://github.com/PKUHaoWang/SpikePingpong" class="btn-accent"><i class="fab fa-github"></i> Code</a>
    </div>
  </div>
</div>

<div class='paper-box floating-card' data-tags="First/Co-First Author, NeuroAI, Transfer Learning, Conference">
  <div class='paper-box-image'>
    <div class="badge pulse-accent">AAAI 2026 Oral</div>
    <img src='images/moase.png' alt="MOASE Overview" width="100%">
  </div>
  <div class='paper-box-text'>
    <h3>Decomposing the Neurons: Activation Sparsity via Mixture of Experts for Continual Test Time Adaptation</h3>
    <div class="authors">Rongyu Zhangâ­ï¸, Aosong Chengâ­ï¸, Yulin Luoâ­ï¸, <span class="primary-gradient-text">Gaole Dai</span>â­ï¸, Huanrui Yang, Jiaming Liu, Ran Xu, Li Du, Yuan DuğŸ“§, Yanbing Jiang, Shanghang ZhangğŸ“§</div>
    <div class="venue">The Fortieth AAAI Conference on Artificial Intelligence (AAAI 2026)</div>
    <div class="links">
      <a href="https://arxiv.org/pdf/2405.16486v1" class="btn-accent"><i class="fas fa-file-alt"></i> Paper</a>
      <a href="https://github.com/RoyZry98/MoASE-Pytorch" class="btn-accent"><i class="fab fa-github"></i> Code</a>
    </div>
  </div>
</div>

<div class='paper-box floating-card' data-tags="Robotics, NeuroAI, Conference">
  <div class='paper-box-image'>
    <div class="badge pulse-accent">AAAI 2026 Poster</div>
    <img src='images/mole-vla.png' alt="MOLE-VLA Overview" width="100%">
  </div>
  <div class='paper-box-text'>
    <h3>MoLe-VLA: Dynamic Layer-skipping Vision Language Action Model via Mixture-of-Layers for Efficient Robot Manipulation</h3>
    <div class="authors">Rongyu Zhang, Menghang Dong, Yuan Zhang, Liang Heng, Xiaowei Chi, <span class="primary-gradient-text">Gaole Dai</span>, Li Du, Yuan DuğŸ“§, Shanghang ZhangğŸ“§</div>
    <div class="venue">The Fortieth AAAI Conference on Artificial Intelligence (AAAI 2026)</div>
    <div class="links">
      <a href="https://arxiv.org/pdf/2503.20384" class="btn-accent"><i class="fas fa-file-alt"></i> Paper</a>
      <a href="https://github.com/RoyZry98/MoLe-VLA-Pytorch/" class="btn-accent"><i class="fab fa-github"></i> Code</a>
    </div>
  </div>
</div>

<div class='paper-box floating-card' data-tags="First/Co-First Author, AI for Life Science, Representation Learning, Journal">
  <div class='paper-box-image'>
    <div class="badge pulse-accent">Nature Computational Science</div>
    <img src='images/inif.png' alt="INIF Overview" width="100%">
  </div>
  <div class='paper-box-text'>
    <h3>Implicit neural image field for biological microscopy image compression</h3>
    <div class="authors"><span class="primary-gradient-text">Gaole Dai</span>â­ï¸, Rongyu Zhang, Qingpo Wuwu, Cheng-Ching Tseng, Yu Zhou, Shaokang Wang, Siyuan Qian, Ming Lu, Ali Ata Tuz, Matthias Gunzer, Tiejun HuangğŸ“§, Jianxu ChenğŸ“§, Shanghang ZhangğŸ“§</div>
    <div class="venue">Nature Computational Science (2026), Q1, IF=18.4</div>
    <div class="links">
      <a href="https://www.nature.com/articles/s43588-025-00889-4" class="btn-accent"><i class="fas fa-file-alt"></i> Paper</a>
      <a href="https://github.com/PKU-HMI/INIF" class="btn-accent"><i class="fab fa-github"></i> Code</a>
    </div>
  </div>
</div>

<div class='paper-box floating-card' data-tags="First/Co-First Author, AI for Life Science, Representation Learning, Conference">
  <div class='paper-box-image'>
    <div class="badge pulse-accent">NeurIPS 2025 Spotlight</div>
    <img src='images/orochi.png' alt="Orochi Overview" width="100%">
  </div>
  <div class='paper-box-text'>
    <h3>Orochi: Versatile Biomedical Image Processor</h3>
    <div class="authors"><span class="primary-gradient-text">Gaole Dai</span>â­ï¸, Chenghao Zhouâ­ï¸, Yu Zhouâ­ï¸, Rongyu Zhang, Yuan Zhang, Chengkai Hou, Tiejun Huang, Jianxu ChenğŸ“§, Shanghang ZhangğŸ“§</div>
    <div class="venue">The Thirty-ninth Annual Conference on Neural Information Processing Systems (NeurIPS 2025)</div>
    <div class="links">
      <a href="https://openreview.net/pdf?id=Rtd6GoJcoT" class="btn-accent"><i class="fas fa-file-alt"></i> Paper</a>
      <a href="https://github.com/daviddaiiiii/Orochi-Versatile-Biomedical-Image-Processor" class="btn-accent"><i class="fab fa-github"></i> Code</a>
    </div>
  </div>
</div>

<div class='paper-box floating-card' data-tags="Transfer Learning, Unified Model, Conference">
  <div class='paper-box-image'>
    <div class="badge pulse-accent">NeurIPS 2025 Poster</div>
    <img src='images/unictoken.png' alt="UniCToken Overview" width="100%">
  </div>
  <div class='paper-box-text'>
    <h3>UniCTokens: Boosting Personalized Understanding and Generation via Unified Concept Tokens</h3>
    <div class="authors">Ruichuan Anâ­ï¸, Sihan Yangâ­ï¸, Renrui Zhang, Zijun Shen, Ming Lu, <span class="primary-gradient-text">Gaole Dai</span>, Hao Liang, Ziyu Guo, Shilin Yan, Yulin Luo, Bocheng Zou, Chaoqun Yang, Wentao ZhangğŸ“§</div>
    <div class="venue">The Thirty-ninth Annual Conference on Neural Information Processing Systems (NeurIPS 2025)</div>
    <div class="links">
      <a href="https://arxiv.org/pdf/2505.14671" class="btn-accent"><i class="fas fa-file-alt"></i> Paper</a>
      <a href="https://github.com/arctanxarc/UniCTokens" class="btn-accent"><i class="fab fa-github"></i> Code</a>
    </div>
  </div>
</div>

<div class='paper-box floating-card' data-tags="First/Co-First Author, Transfer Learning, NeuroAI, Conference">
  <div class='paper-box-image'>
    <div class="badge pulse-accent">ICML 2025 Poster</div>
    <img src='images/san.png' alt="SAN Overview" width="100%">
  </div>
  <div class='paper-box-text'>
    <h3>SAN: Hypothesizing Long-Term Synaptic Development and Neural Engram Mechanism in Scalable Model's Parameter-Efficient Fine-Tuning</h3>
    <div class="authors"><span class="primary-gradient-text">Gaole Dai</span>â­ï¸, Chun-Kai Fanâ­ï¸, Yiming Tang, Zhi Zhang, Yuan Zhang, Yulu Gan, Qizhe Zhang, Cheng-Ching Tseng, Shanghang ZhangğŸ“§, Tiejun HuangğŸ“§</div>
    <div class="venue">The Forty-second International Conference on Machine Learning (ICML 2025)</div>
    <div class="links">
      <a href="https://arxiv.org/pdf/2409.06706" class="btn-accent"><i class="fas fa-file-alt"></i> Paper</a>
      <a href="https://github.com/daviddaiiiii/SAN-PEFT" class="btn-accent"><i class="fab fa-github"></i> Code</a>
    </div>
  </div>
</div>

<div class='paper-box floating-card' data-tags="First/Co-First Author, AI for Life Science, Journal">
  <div class='paper-box-image'>
    <div class="badge pulse-accent">Nature Methods</div>
    <img src='images/gbai.png' alt="GBAI Overview" width="100%">
  </div>
  <div class='paper-box-text'>
    <h3>Multimodal large language models for bioimage analysis</h3>
    <div class="authors">Shanghang ZhangğŸ“§, <span class="primary-gradient-text">Gaole Dai</span>â­ï¸, Tiejun Huang, Jianxu ChenğŸ“§</div>
    <div class="venue">Nature Methods, Q1, IF=32.1</div>
    <div class="links">
      <a href="https://www.nature.com/articles/s41592-024-02334-2" class="btn-accent"><i class="fas fa-file-alt"></i> Paper</a>
      <a href="https://www.nature.com/articles/s41592-024-02334-2" class="btn-accent"><i class="fab fa-github"></i> Code</a>
    </div>
  </div>
</div>

<span class='anchor' id='-awards'></span>
# ğŸ† Awards
- *2025.01 - 2027.01*: &nbsp;Natural Science Foundation of China, Pilot Program of Research <span class="primary-gradient-text">Fund</span> for Excellent PhD Students, 100,000Â¥/year.
- *2022.06 - Present*: &nbsp;Chinese Government <span class="primary-gradient-text">Scholarship</span> for Excellent PhD Students
- *2023.09*: &nbsp;International Conference On Computer Vision (ICCV) 2023, Continue Test-time Adaptation for Semantic Segmentation <span class="primary-gradient-text">Challenge</span>, Sliver Place.  

<span class='anchor' id='-interships'></span>
# ğŸ’¼ Internships
- *2024.08 - Present*: &nbsp;Research Intern, <a href="https://www.baai.ac.cn" class="link-accent">Beijing Academy of Artifical Intelligence (BAAI)</a>.
- *2024.01 - 2024.08*: &nbsp;Research Intern, <a href="https://www.intellif.com/" class="link-accent">Intellifusion Technology</a>.

<span class='anchor' id='-interests'></span>
# ğŸ˜Š Interests
<div class="blog-grid">
  <div class="blog-card">
    <div class="blog-card-image">
      <div class="blog-badge">My Favorite Anime Studio</div>
      <img src="images/kyoani.png" alt="kyoani">
    </div>
    <div class="blog-card-content">
      <div class="blog-title">Kyoto Animation</div>
      <div class="blog-description">Since inauguration, our principles are "Make a challenge", "Do the best", "Produce required works" and "Keep our corporate as a humanitarian one". We value people. Promoting the growth of people is equal to creating the brightness of works. We sincerely keep moving forward to be an Entertainment Corporation which based on Animation.</div>
      <div class="blog-links">
        <a href="https://www.kyotoanimation.co.jp/en/" class="blog-link">
          <i class="fas fa-globe"></i> Link
        </a>
      </div>
    </div>
  </div>

<div class="blog-grid">
  <div class="blog-card">
    <div class="blog-card-image">
      <div class="blog-badge">My Favorite Anime Studio</div>
      <img src="images/kyoani.png" alt="kyoani">
    </div>
    <div class="blog-card-content">
      <div class="blog-title">Kyoto Animation</div>
      <div class="blog-description">Since inauguration, our principles are "Make a challenge", "Do the best", "Produce required works" and "Keep our corporate as a humanitarian one". We value people. Promoting the growth of people is equal to creating the brightness of works. We sincerely keep moving forward to be an Entertainment Corporation which based on Animation.</div>
      <div class="blog-links">
        <a href="https://www.kyotoanimation.co.jp/en/" class="blog-link">
          <i class="fas fa-globe"></i> Link
        </a>
      </div>
    </div>
  </div>


<span class='anchor' id='-others'></span>
# ğŸ’¬ Others


<script>
document.addEventListener('DOMContentLoaded', function() {
  const filterContainer = document.getElementById('filter-container');
  const paperBoxes = document.querySelectorAll('.paper-box');
  let activeTags = new Set();
  let allTags = new Set();

  // 1. æ‰«ææ‰€æœ‰è®ºæ–‡ï¼Œæå–æ‰€æœ‰ä¸é‡å¤çš„æ ‡ç­¾
  paperBoxes.forEach(box => {
    const tags = box.getAttribute('data-tags');
    if (tags) {
      tags.split(',').forEach(tag => {
        allTags.add(tag.trim());
      });
    }
  });

  // 2. å°†æ ‡ç­¾æ’åºå¹¶ç”ŸæˆæŒ‰é’®
  const sortedTags = Array.from(allTags).sort();
  
  // æ·»åŠ  "All" æŒ‰é’®ï¼ˆå¯é€‰ï¼Œæˆ–è€…ç”¨æ¸…é™¤åŠŸèƒ½ï¼‰
  // è¿™é‡Œæˆ‘ä»¬é‡‡ç”¨ç‚¹å‡»æ ‡ç­¾è¿›è¡Œ toggle çš„æ–¹å¼ï¼Œä¸é€‰ä»»ä½•æ ‡ç­¾å³æ˜¾ç¤ºå…¨éƒ¨

  sortedTags.forEach(tag => {
    const btn = document.createElement('button');
    btn.className = 'filter-btn';
    btn.textContent = tag;
    
    // 3. æŒ‰é’®ç‚¹å‡»äº‹ä»¶
    btn.onclick = () => {
      // åˆ‡æ¢é€‰ä¸­çŠ¶æ€
      if (activeTags.has(tag)) {
        activeTags.delete(tag);
        btn.classList.remove('active');
      } else {
        activeTags.add(tag);
        btn.classList.add('active');
      }
      
      filterPapers();
    };
    
    filterContainer.appendChild(btn);
  });

  // 4. æ ¸å¿ƒè¿‡æ»¤é€»è¾‘
  function filterPapers() {
    paperBoxes.forEach(box => {
      const boxTagsString = box.getAttribute('data-tags');
      
      // å¦‚æœæ²¡æœ‰é€‰ä¸­ä»»ä½•æ ‡ç­¾ï¼Œæ˜¾ç¤ºæ‰€æœ‰
      if (activeTags.size === 0) {
        box.classList.remove('hidden');
        return;
      }

      // å¦‚æœå¡ç‰‡æ²¡æœ‰æ ‡ç­¾ï¼Œä½†åœ¨ç­›é€‰æ¨¡å¼ä¸‹ï¼Œç›´æ¥éšè—
      if (!boxTagsString) {
        box.classList.add('hidden');
        return;
      }

      const boxTags = boxTagsString.split(',').map(t => t.trim());
      
      // é€»è¾‘ï¼šå¿…é¡»åŒ…å«æ‰€æœ‰é€‰ä¸­çš„æ ‡ç­¾ (AND é€»è¾‘)
      // å¦‚æœä½ æƒ³è¦åªè¦åŒ…å«å…¶ä¸­ä¸€ä¸ªå°±æ˜¾ç¤º (OR é€»è¾‘)ï¼Œè¯·æŠŠ every æ”¹æˆ some
      const isVisible = Array.from(activeTags).every(activeTag => boxTags.includes(activeTag));

      if (isVisible) {
        box.classList.remove('hidden');
      } else {
        box.classList.add('hidden');
      }
    });
  }
});
</script>
